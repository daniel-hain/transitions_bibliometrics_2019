---
title: "Transitions Bibliometrics 2019"
author: "Daniel S. Hain"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes

---

```{r setup, include=FALSE}
### Generic preamble
Sys.setenv(LANG = "en")
options(scipen = 5)
set.seed(1337)

### Load packages  
library(knitr) # For display of the markdown
library(tidyverse)
library(magrittr)

library(kableExtra) # For table styling
library(tidygraph)
library(ggraph)

# own functions
source("../functions/functions_basic.R")
source("../functions/functions_summary.R")
source("../functions/00_parameters.R")

```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
```


<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Initial Corpus generation & inspection 

```{r}
M <- readRDS("../../temp/M.RDS") %>% as_tibble()
M_bib <- readRDS("../../temp/M_bib.RDS") %>% as_tibble()

M %<>% semi_join(M_bib, by = 'XX')
```


## Corpus Creation
1. Scopus download of documents retrieved from search string from Markard et al. (2012). Limited to `LANGUAGE = ENGLISH AND TYPE = (ARTICLE)`.
2. Selecting "seed" publications. 1% most cited per year. Ex-post manual exclusion. Results in 53 seed papers
3. Retrieving for each seed 1000 publications with most shared references. Again, same limitations as in step 1. 
4. Adittional ex. post filtering. First, based on citations recieved and connectivity in bibliographic coupling network. Namely, I excluded edges in the bottom 10% quantile of the weight distribution (Jaccard weighted), also unconnected and nodes in the bottom 10% of the degree distribution. Lastly,after the community detection exercise, I excluded nodes in communities of less than 500 members. 

That leads to an overall corpus size of: 
```{r}
cat("Number of unique publications in the final corpus: ", nrow(M))
```

## Seed Paper {.tabset}
```{r}
# NOTE: NOT WORKING TILL WE GET THE SEED PAPER LIST

# M_seed <- M %>%
#   filter(UT %in% seed_UT) %>%
#   mutate(TI = TI%>% str_trunc(300),
#          SO = SO %>% str_trunc(50),
#          TC_year = (TC / (2019 - PY + 1)) %>% round())  %>%
#   select(AU, PY, TI, SO, TC, TC_year) %>%
#   arrange(desc(TC_year))
# 
# 
# seed %<>% mutate(n = 1:n(),
#                  SR = paste0(n, ": ", AU, ", (", PY, ") ", (SO %>% str_trunc(50)), ", Cited: ", TC, "Cited/year: ", TC_year)) %>% 
#   select(n, SR, everything())
# 
# seed[1, "SR"] <- seed[1, "TI"]
```

In the following, we more in detail investigate the seed papers. Notice the adittional `tabs` for details on ou selection of seed papers.

### Seep papers and corpus size

Generally, 50 x 1000 = 50.000 documents downloaded. However, due to an overlap of publications with most shared references to seed papers, final corpus is smaller. 

```{r, fig.height=5, fig.width=10}
# TODO: Add paper name
plot <- M %>%
  count(batch) %>%
  arrange(batch) %>%
  mutate(n_corpus = n %>% cumsum()) %>%
  ggplot( aes(x = batch, y = n_corpus, text = batch) ) +
  geom_point() +
  scale_color_brewer(palette="Dark2")

plot %>% plotly::ggplotly(tooltip = c("x", "y") )
```

First insight: It appears the main Sustainability corpus seems saturated, expansion appears more in adjacent fields.

### List of all seed papers

NEEDS UPDATE

```{r}
# seed %>% 
#   select(-SR, -index) %>%
#   select(n, everything()) %>%
#   kable(row.names = FALSE) %>% 
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, font_size = 10) 
```

## Publications
Note: The following tables refer to the documents in the main corpus. Generally, `dgr` refers to the degree, `n` to the number of publications. Subscript `.f` indicates the number is fractionalized (divided by the number of elements per publication)

```{r}
#M %>% gen_summary(top_n = 20, level = "PUB", what = "count", plot = TRUE) 
```
