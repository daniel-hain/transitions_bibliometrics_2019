---
title: "Transitions Bibliometrics 2020"
author: "Daniel S. Hain"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
---

```{r setup, include=FALSE}
### Generic preamble
Sys.setenv(LANG = "en")
options(scipen = 5)
set.seed(1337)

### Load packages  
library(knitr) # For display of the markdown
library(kableExtra) # For table styling

library(tidyverse)
library(magrittr)


library(bibliometrix)
library(tidygraph)
library(ggraph)

# own functions
source("../functions/functions_basic.R")
source("../functions/functions_summary.R")
source("../functions/00_parameters.R")

```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
```


<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Initial Corpus generation 

```{r}
M <- readRDS("../../temp/M.RDS") %>% as_tibble()
M_bib <- readRDS("../../temp/M_bib.RDS") %>% as_tibble()

M %<>% semi_join(M_bib, by = 'XX')
```


## Corpus Creation
1. Scopus download of documents retrieved from search string from Markard et al. (2012). Limited to `LANGUAGE = ENGLISH AND TYPE = (ARTICLE)`.
2. Selecting "seed" publications. 1% most cited per year. Ex-post manual exclusion. Results in 53 seed papers
3. Retrieving for each seed 1000 publications with most shared references. Again, same limitations as in step 1. 
4. Adittional ex. post filtering. First, based on citations recieved and connectivity in bibliographic coupling network. Namely, I excluded edges in the bottom 10% quantile of the weight distribution (Jaccard weighted), also unconnected and nodes in the bottom 10% of the degree distribution. Lastly,after the community detection exercise, I excluded nodes in communities of less than 500 members. 

That leads to an overall corpus size of: 
```{r}
cat("Number of unique publications in the final corpus: ", nrow(M))
```

## Seed Paper
```{r}
# NOTE: NOT WORKING TILL WE GET THE SEED PAPER LIST

# M_seed <- M %>%
#   filter(UT %in% seed_UT) %>%
#   mutate(TI = TI%>% str_trunc(300),
#          SO = SO %>% str_trunc(50),
#          TC_year = (TC / (2019 - PY + 1)) %>% round())  %>%
#   select(AU, PY, TI, SO, TC, TC_year) %>%
#   arrange(desc(TC_year))
# 
# 
# seed %<>% mutate(n = 1:n(),
#                  SR = paste0(n, ": ", AU, ", (", PY, ") ", (SO %>% str_trunc(50)), ", Cited: ", TC, "Cited/year: ", TC_year)) %>% 
#   select(n, SR, everything())
# 
# seed[1, "SR"] <- seed[1, "TI"]
```

In the following, we more in detail investigate the seed papers. 

### List of all seed papers

NEEDS UPDATE

```{r}
# seed %>% 
#   select(-SR, -index) %>%
#   select(n, everything()) %>%
#   kable(row.names = FALSE) %>% 
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, font_size = 10) 
```

### Seep papers and corpus size

Generally, 50 x 1000 = 50.000 documents downloaded. However, due to an overlap of publications with most shared references to seed papers, final corpus is smaller. 

```{r, fig.height=5, fig.width=10}
# TODO: Add paper name
plot <- M %>%
  count(batch) %>%
  arrange(batch) %>%
  mutate(n_corpus = n %>% cumsum()) %>%
  ggplot( aes(x = batch, y = n_corpus, text = batch) ) +
  geom_point() +
  scale_color_brewer(palette="Dark2")

plot %>% plotly::ggplotly(tooltip = c("x", "y") )
```

First insight: It appears the main Sustainability corpus seems saturated, expansion appears more in adjacent fields.

<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# General Overview over the ST and surrounding fields

## Main Indicators: Publications, Authors, Countries

To start with, a general overview over the documents in the corpus

```{r}
results<- readRDS("../../temp/results.RDS") 

results %>% summary(k = 10, pause = FALSE)
```

```{r}
results %>% plot(k = 10, pause = FALSE)
```

```{r}
rm(results)
```

## Cited references

```{r}
CR <- readRDS("../../temp/CR.RDS") 
```

Top 20 cited references:

```{r}
CR$Cited %>% as_tibble() %>% head(20)
```

```{r}
rm(CR)
```

```{r}
#M %>% gen_summary(top_n = 20, level = "PUB", what = "count", plot = TRUE) 
```

## Conceptual trajectories: Historical citation path analysis

```{r}
histResults <- readRDS("../../temp/histResults.RDS") 
```

```{r, fig.width=20, fig.height=17.5}
histResults %>% histPlot(n = 25, size = 10, labelsize = 5)
```


```{r}
rm(histResults)
```

### Authors, Themes & Journals

```{r, fig.width=20, fig.height=17.5}
M_threefield <- readRDS("../../temp/M_threefield.RDS") 
```

```{r, fig.width=20, fig.height=17.5}
M_threefield
```


```{r}
rm(M_threefield)
```


<!-- ####################################################################################### -->
<!-- ####################################################################################### -->
<!-- ############################# NEXT PART ############################################### -->
<!-- ####################################################################################### -->
<!-- ####################################################################################### -->

# Topic modelling
```{r}
library(tidytext)

text_tidy <- readRDS("../../temp/text_tidy.RDS")
text_lda <- readRDS("../../temp/text_lda.RDS")
```

I by now created some topic modelling. The results are now more fine-tuned, but there is still room for some improvement. We ran a LDA on the titles + abstracts of our corpus, aiming at identifying 10 topics (some different numbers of topics to generate shows that 10 result in good results, more topics lead to too much overlap between them)

## Topics by topwords
```{r, fig.width=20, fig.height=17.5} 
# , fig.width=20, fig.height=17.5
mycol_lda <- text_lda %>% tidy(matrix = "beta") %>% gg_color_select(cat = topic, pal = "Paired")

text_lda %>% tidy(matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = "Intra-topic distribution of word",
       y = "Words in topic") + 
  scale_fill_manual(name = "Legend", values = mycol_lda) 

#plot_ly <- plot %>% plotly::ggplotly()
#htmlwidgets::saveWidget(plotly::as_widget(plot_ly), '../output\vis_plotly_topic_terms.html', selfcontained = TRUE)
```

This might still be finetuned, but initially doesnt look that bad I think. All the topics for me seem to be somewhat identifiable. We should maybe start naming them to make their interpretation later easier. 


```{r, fig.width = 15, fig.height=7.5}

# ## Topics over time
# text_lda %>% tidy(matrix = "gamma") %>%
#   rename(weight = gamma) %>%
#   left_join(M %>% select(XX, PY), by = c('document' = 'XX')) %>%
#   mutate(PY = as.numeric(PY)) %>%
#   group_by(PY, topic) %>% summarise(weight = sum(weight)) %>% ungroup() %>%
#   group_by(PY) %>% mutate(weight_PY = sum(weight)) %>% ungroup() %>%
#   mutate(weight_rel = weight / weight_PY) %>%
#   select(PY, topic, weight, weight_rel) %>%
#   filter(PY >= 1998 & PY <= 2019) %>%
#   arrange(PY, topic) %>%
#   plot_summary_timeline(y1 = weight, y2 = weight_rel, t = PY, by = topic,  pal = "Paired", label = FALSE, 
#                         y1_text = "Topic popularity annualy", y2_text = "Share of topic annually")
```

## LDAViz
Here you find a nice way of exploring topics via the `LDAVIz` methodology of visulizing the result of an LDA. It dispolays all topics in a 2 dimensional TSNE (similar to PCA, but optimized for graphical illustration in 2d), and also gives a nice visual representation over the topics top-word distribution and overall frequencies of this words in the corpus. The $\lambda$ parameter regulates the importance-ordering of the topwords. High $\lambda$ order words by the highest propability to appear in the topic to the lowest (independent of the overall word popularity in the corpus), whle low $\lambda$ emphasize words which are very specific to the topic, and rarely appear in others.

Play a bit around. Since it would be here a bit condensed, better check it out  [HERE](https://daniel-hain.github.io/transitions_bibliometrics_2019/output/LDAviz/) in fullscreen for a better overview.

